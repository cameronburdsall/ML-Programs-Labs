# -*- coding: utf-8 -*-
"""Program1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11L28WSUZptlEAo7bgmx5GEJe3LNOzgpE
"""

# imports
import numpy as np
import pandas as pd
import scipy.sparse as sp
import re, sys
from collections import Counter, defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics import f1_score
from scipy.sparse import csr_matrix
from nltk.corpus import stopwords
from scipy.spatial.distance import cosine
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

"""Put Data into structures"""

#filepaths

train_path = 'train.dat'
test_path = 'test.dat'

#put data into frame
with open(train_path, 'r') as f:
  train_lines = f.readlines()

with open(test_path, 'r') as f:
  test_lines = f.readlines()

train_data = [l.split() for l in train_lines]
test_data = [l.split() for l in test_lines]

train_classes = []
train_documents = []
test_documents = []

for i in range (len(train_data)):
  train_classes.append(train_data[i][0])
  train_documents.append(train_data[i][1:])

for i in range (len(test_data)):
  test_documents.append(test_data[i][0:])

#train data
train_df = pd.DataFrame()
train_df['Class'] = train_classes
train_df['Text'] = train_documents

#test data
test_df = pd.DataFrame()
test_df['Text'] = test_documents

def has_nums(string):
  #regex to find any numbers
  return re.search(r'\d', string)

def convert_lowercase (docs):
  #converts lowercase and removes numbers
  nd = []
  for doc in docs:
    words = []
    #for each word in curent document
    for word in doc:
      if has_nums(word):
        continue
      elif not word.islower():
        words.append(word.lower())
      else:
        words.append(word)
    nd.append(words)
  return nd

train_df['Text'] = convert_lowercase(train_df['Text'])
test_df['Text'] = convert_lowercase(test_df['Text'])

def remove_useless_words (docs):
  nd = []
  bad_words = set(stopwords.words('english'))
  for doc in docs:
    words = []
    #for each word in curent document
    for word in doc:
      if word not in bad_words:
        words.append(word)
    nd.append(words)
  return nd

train_df['Text'] = remove_useless_words(train_df['Text'])
test_df['Text'] = remove_useless_words(test_df['Text'])

def remove_punc (docs):
  #for each document
  nd = []
  for doc in docs:
    words = []
    #for each word in curent document
    for word in doc:
      #regex for finding and subbing punctuation with empty space
      temp = re.sub(r'[^\w\s]', '', word)
      if word != '':
        words.append(temp)
    nd.append(words)
  return nd

#remove punctuation from the dataset
train_df['Text'] = remove_punc(train_df['Text'])
test_df['Text'] = remove_punc(test_df['Text'])

def filterLen(docs, minlen):
    r""" filter out terms that are too short. 
    docs is a list of lists, each inner list is a document represented as a list of words
    minlen is the minimum length of the word to keep
    """
    return [ [t for t in d if len(t) >= minlen ] for d in docs ]

train_df['Text'] = filterLen(train_df['Text'], 3)
test_df['Text'] = filterLen(test_df['Text'], 3)

def combine_lemmas(docs):
  nd = []
  for doc in docs:
    words = []
    #for each word in curent document
    for word in doc:
      #regex for finding and subbing punctuation with empty space
      temp = WordNetLemmatizer().lemmatize(word, pos='v')
      words.append(temp)
    nd.append(words)
  return nd

train_df['Text'] = combine_lemmas(train_df['Text'])
test_df['Text'] = combine_lemmas(test_df['Text'])

def csr_info(mat, name="", non_empy=False):
    r""" Print out info about this CSR matrix. If non_empy, 
    report number of non-empty rows and cols as well
    """
    if non_empy:
        print("%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]" % (
                name, mat.shape[0], 
                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 
                for i in range(mat.shape[0])), 
                mat.shape[1], len(np.unique(mat.indices)), 
                len(mat.data)))
    else:
        print( "%s [nrows %d, ncols %d, nnz %d]" % (name, 
                mat.shape[0], mat.shape[1], len(mat.data)) )

def csr_l2normalize(mat, copy=False, **kargs):
    r""" Normalize the rows of a CSR matrix by their L-2 norm. 
    If copy is True, returns a copy of the normalized matrix.
    """
    if copy is True:
        mat = mat.copy()
    nrows = mat.shape[0]
    nnz = mat.nnz
    ind, val, ptr = mat.indices, mat.data, mat.indptr
    # normalize
    for i in range(nrows):
        rsum = 0.0    
        for j in range(ptr[i], ptr[i+1]):
            rsum += val[j]**2
        if rsum == 0.0:
            continue  # do not normalize empty rows
        rsum = 1.0/np.sqrt(rsum)
        for j in range(ptr[i], ptr[i+1]):
            val[j] *= rsum
            
    if copy is True:
        return mat

def build_matrix(docs):
    r""" Build sparse matrix from a list of documents, 
    each of which is a list of word/terms in the document.  
    """
    nrows = len(docs)
    idx = {}
    tid = 0
    nnz = 0
    for d in docs:
        nnz += len(set(d))
        for w in d:
            if w not in idx:
                idx[w] = tid
                tid += 1
    ncols = len(idx)
        
    # set up memory
    ind = np.zeros(nnz, dtype=np.int)
    val = np.zeros(nnz, dtype=np.double)
    ptr = np.zeros(nrows+1, dtype=np.int)
    i = 0  # document ID / row counter
    n = 0  # non-zero counter
    # transfer values
    for d in docs:
        cnt = Counter(d)
        keys = list(k for k,_ in cnt.most_common())
        l = len(keys)
        for j,k in enumerate(keys):
            ind[j+n] = idx[k]
            val[j+n] = cnt[k]
        ptr[i+1] = ptr[i] + l
        n += l
        i += 1
            
    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)
    mat.sort_indices()
    
    return mat

train_size = len(train_df)
all_text = pd.concat([train_df['Text'], test_df['Text']])

#build full word-document matrix
big_mat = build_matrix(all_text)

csr_l2normalize(big_mat)


#resplit based on train/test set
train_mat = big_mat[:train_size]
test_mat = big_mat[train_size:]

def cos_distance(doc1, doc2):
  sim = cosine_similarity(doc1, doc2)
  dist = 1 - sim
  dist *= dist
  return dist

twothirds = int(train_mat.shape[0] * 4 / 5)
trainer_mat = train_mat[:twothirds]
tester_mat = train_mat[twothirds:]
trainer_ans = train_df['Class'][:twothirds]
tester_ans = train_df['Class'][twothirds:]

def most_frequent(lst):
  occ = Counter(lst)
  return occ.most_common(1)[0][0]

def ranked_voting(preds, weights):
  """
  Returns the most heavily weighted prediction
  preds - array of predictions
  weights - array of weights
  """
  votes = {}
  vals = set(preds)
  for v in vals:
    votes[v] = 0
  for i in range (len(preds)):
    votes[preds[i]] += weights[i]
  v = list(votes.values())
  k = list(votes.keys())
  return k[v.index(max(v))]

def dot_distance(train, test):
  """
  Distance generated by dot product
  train - training set
  test - testing set
  """
  dots = test.dot(train.T)
  return dots.data

def findKNN(train, test, classes, k, distance = 'Euclidean', voting = 'Majority'):
  """
  train - Training Set
  test - Testing Set
  k - Number of Nearest Neighors to find
  distance (optional) - Select which distance measure to use,
                        Can be Euclidean(default) or Cosine
  voting (optional) - Select class voting method,
                      Can be Majority(default) or Weighted
  """
  predictions = []
  #print(classes[0])
  for te_entry in test:
    closest = -1
    best = 0
    sims = []
    indicies = []

    dist = None
    if distance == 'Euclidean':
      dist = euclidean_distances(train, te_entry)
    elif distance == 'Cosine':
      dist = cos_distance(train, te_entry)
    elif distance == 'Dot':
      dist = dot_distance(train, te_entry)
    else:
      raise Exception("Invalid Distance parameter")
    indices = list(range(0, len(dist)))

    tes = list(zip(indices, dist))
    tes.sort(key = lambda x: x[1], reverse=False)
    cls = []
    w = []

    for i in range (k):
      cls.append(classes[tes[i][0]])
      if tes[i][1] == 0:
        w.append(sys.maxsize)
      else:
        w.append(1/((tes[i][1])*(tes[i][1])))

    if voting == 'Majority':
      predictions.append(most_frequent(cls))
    elif voting == 'Weighted':
      predictions.append(ranked_voting(cls, w))
      #print(ranked_voting(cls, w))
      #print(w)
  return predictions


pred = findKNN(train_mat, test_mat, train_df['Class'], 150, voting = 'Weighted', distance='Cosine')

test_pred_file = open ('predictions.dat', 'w+')
ans_df = pd.DataFrame()
ans_df['Results'] = pred
ans_df.to_csv('predictions.dat', index=False, header=None)
